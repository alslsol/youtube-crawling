from konlpy.tag import Kkma
from konlpy.utils import pprint
from collections import Counter
import os
import pandas as pd

# ê³ ìœ ëª…ì‚¬ ëª©ë¡
custom_exceptions = [
    'ê°•ì°¬ì„', 'ì§€ë°•ë ¹', 'ê¹€ê·œë‚¨', 'ì¸ìŠ¤íƒ€', 'ìœ íŠœë¸Œ', 'ì˜¤ì¦ˆëª¨', 'ì£¼í˜„ì˜', 'ë‚¨ì¤‘ê·œ', 'ì„œìˆ˜ë¯¼',
    'í”„ë¡œëª¨ì…˜', 'í”„ë¡œì•¼êµ¬', 'í›ˆë‚¨', 'ê¸€ë¡œìŠ¤', 'íˆ¬ì–´ìŠ¤', 'ì†ë§ˆìŒ'
]

# ì¶”ì¶œí•  í’ˆì‚¬
allowed_pos = {'NNG', 'NNP'}

# ë¶ˆìš©ì–´ ëª©ë¡
custom_stopwords = set([
    'ì˜', 'ê°€', 'ì´', 'ì€', 'ë“¤', 'ëŠ”', 'ì¢€', 'ì˜', 'ê±', 'ê³¼', 'ë„', 'ë¥¼',
    'ìœ¼ë¡œ', 'ì', 'ì—', 'ì™€', 'í•œ', 'í•˜ë‹¤', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ê³ ', 'ë„',
    'ì…ë‹ˆë‹¤', 'ê·¸ë¦¬ê³ ', 'ê·¸', 'ê²ƒ', 'ìˆ˜', 'ë˜í•œ', 'ìˆëŠ”', 'ì—†ëŠ”', '(', ')', '.',
    '!', '?', 'ê¸€', 'ì˜¤ëŠ˜', 'ì˜ìƒ', 'ì§€ê¸ˆ', 'ë¬¸ì˜', 'ê°ì‚¬', 'ã…œã…œ', 'ã… ã… ', 'ã…‹ã…‹',
    'ë§í¬', 'ì¸', 'ìŠ¤íƒ€', 'íˆ¬', 'ìœ ', 'ì œì‘', 'ì°¸ì—¬', 'ì´ë²¤íŠ¸', 'ê¹€', 'ì „', 'ì‚¬ìš©',
])

# ê³ ìœ ëª…ì‚¬ ë³´í˜¸ í•¨ìˆ˜
def preserve_proper_nouns(text, proper_nouns):
    if not isinstance(text, str):
        text = str(text) if isinstance(text, float) else ''  # float íƒ€ì…ì€ strë¡œ ë³€í™˜, ê·¸ ì™¸ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬

    # ê³ ìœ ëª…ì‚¬ì— _ ë„£ì–´ì„œ ë³´í˜¸
    for phrase in proper_nouns:
        if phrase in text:
            text = text.replace(phrase, phrase + "_")
    return text

# def restore_proper_nouns(tokens):
#     # NaN ì²˜ë¦¬ ë° ë¬¸ìì—´ë¡œ ë³€í™˜
#     if not isinstance(text, str):
#         text = str(text) if isinstance(text, float) else ''  # float íƒ€ì…ì€ strë¡œ ë³€í™˜, ê·¸ ì™¸ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬

#     for phrase in proper_nouns:
#         if phrase in text:
#             text = text.replace(phrase, phrase.replace(" ", "_"))
#     return text

def get_file_list():
    """
    íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    :return: íŒŒì¼ ëª©ë¡ (list of strings)
    """
    data_folder = os.path.join(os.path.dirname(__file__), 'data')
    
    if not os.path.exists(data_folder):
        print(f"'{data_folder}' í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []
    
    # íŒŒì¼ ëª©ë¡ ì½ê¸°
    file_list = os.listdir(data_folder)
    return file_list


def csv_to_dataframe(file_name):
    """
    :param file_name: CSV íŒŒì¼ ì´ë¦„ (string)

    :return: DataFrame (pandas DataFrame)
    """

    # 'data' í´ë” ê²½ë¡œ ì„¤ì •
    data_folder = os.path.join(os.path.dirname(__file__), 'data')
    file_path = os.path.join(data_folder, file_name)
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(file_path):
        print(f"'{file_path}' íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    
    # CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
    try:
        df = pd.read_csv(file_path)
        return df
    except Exception as e:
        print(f"CSV íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None


def get_column_as_list(dataframe, column_name):
    """
    :param dataframe DataFrame (pandas DataFrame)
    :param column_name ì»¬ëŸ¼ ì´ë¦„ (string)

    :return: ì»¬ëŸ¼ ê°’ ë¦¬ìŠ¤íŠ¸ (list)
    """
    if column_name not in dataframe.columns:
        print(f"'{column_name}' ì»¬ëŸ¼ì´ DataFrameì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []
    
    # ì»¬ëŸ¼ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    return dataframe[column_name].tolist()

# ê³ ìœ ì–´ í•©ì¹˜ëŠ” í•¨ìˆ˜
def merge_proper_nouns(result, allowed_pos, custom_exceptions):
    """
    ê³ ìœ ëª…ì‚¬ë¥¼ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜
    :param result: í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ (list of tuples)
    :param allowed_pos: ë¶„ì„í•  í’ˆì‚¬ (set)
    :return: í•˜ë‚˜ë¡œ í•©ì³ì§„ í† í° ë¦¬ìŠ¤íŠ¸
    """
    merged_words = []
    current_word = ""
    current_pos = ""
    
    for word, pos in result:
        # ê³ ìœ ëª…ì‚¬ ì—°ì†ì´ ìˆìœ¼ë©´ í•˜ë‚˜ë¡œ í•©ì¹¨
        if word in custom_exceptions:
            if current_word == "":
                current_word = word  # ìƒˆë¡œìš´ ë‹¨ì–´ ì‹œì‘
                current_pos = pos
            else:
                current_word += word  # ì—°ì†ëœ ê³ ìœ ëª…ì‚¬ í•©ì¹˜ê¸°
        else:
            # ê³ ìœ ëª…ì‚¬ ì—°ì†ì´ ëë‚¬ìœ¼ë©´ ê¸°ì¡´ ë‹¨ì–´ë¥¼ ì €ì¥í•˜ê³  ì´ˆê¸°í™”
            if current_word:
                merged_words.append((current_word, current_pos))
                current_word = ""
                current_pos = ""
            if word != "_":  # ë³´í˜¸ëœ ê³ ìœ ëª…ì‚¬ "_"ëŠ” ì œì™¸í•˜ê³  ì¶”ê°€
                merged_words.append((word, pos))  # ê³ ìœ ëª…ì‚¬ ì™¸ì˜ ë‹¤ë¥¸ í’ˆì‚¬ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
        
    # ë§ˆì§€ë§‰ì— ë‚¨ì•„ ìˆëŠ” ë‹¨ì–´ ì¶”ê°€
    if current_word:
        merged_words.append((current_word, current_pos))
    
    return merged_words


def analyze_morphology(sentence):
    """
    ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    :param sentence ë¶„ì„í•  ë¬¸ì¥ (string)

    :return: í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ (list of tuples)
    """
    kkma = Kkma()
    try:
        result = kkma.pos(sentence)
        result = [word_pos for word_pos in result if word_pos[1] in allowed_pos]
        return result
    except Exception as e:
        print(f"í˜•íƒœì†Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []


if __name__ == "__main__":
    # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    file_list = get_file_list()
    total_tokens = []

    for file_name in file_list:
        print(f"Processing file: {file_name}")
        df = csv_to_dataframe(file_name)

        titles = get_column_as_list(df, 'title')
        descriptions = get_column_as_list(df, 'description')
        
        for title in titles:
            # print(f"Analyzing title: {title}")

            # ê³ ìœ ëª…ì‚¬ ë³´í˜¸
            title = preserve_proper_nouns(title, custom_exceptions)

            # í˜•íƒœì†Œ ë¶„ì„
            result = analyze_morphology(title)

            # ê³ ìœ ëª…ì‚¬ í•©ì¹˜ê¸°
            merged_words = merge_proper_nouns(result, allowed_pos, custom_exceptions)
            
            words = [
                word if word.endswith('_') else word.replace('_', '') # ê³ ìœ ëª…ì‚¬ í† í°ì€ ìœ ì§€
                for word, pos in merged_words 
                if word.replace('_', '') not in custom_stopwords]
            total_tokens.extend(words)
            # print(merged_words)

        for description in descriptions:
            # print(f"Analyzing description: {description}")

            # ê³ ìœ ëª…ì‚¬ ë³´í˜¸
            description = preserve_proper_nouns(description, custom_exceptions)

            result = analyze_morphology(description)
            
            # ê³ ìœ ëª…ì‚¬ í•©ì¹˜ê¸°
            merged_words = merge_proper_nouns(result, allowed_pos, custom_exceptions)
            
            words = [
                word if word.endswith('_') else word.replace('_', '')
                for word, pos in merged_words
                if word.replace('_', '') not in custom_stopwords]
            total_tokens.extend(words)
            # print(merged_words)

# ë¹ˆë„ ê³„ì‚°
freq = Counter(total_tokens)
print("\nğŸ“Š ìì£¼ ë“±ì¥í•œ ë‹¨ì–´ Top 20:")
for word, count in freq.most_common(20):
    print(f"{word}: {count}íšŒ")

# tf-idf
total_docs = []

for file_name in file_list:
    print(f"Processing file: {file_name}")
    df = csv_to_dataframe(file_name)

    titles = get_column_as_list(df, 'title')
    descriptions = get_column_as_list(df, 'description')

    for title, description in zip(titles, descriptions):
        # --- ë¬¸ì„œ í•˜ë‚˜ ë§Œë“¤ê¸° ---
        tokens = []

        for text in [title, description]:
            text = preserve_proper_nouns(text, custom_exceptions)
            result = analyze_morphology(text)
            merged_words = merge_proper_nouns(result, allowed_pos, custom_exceptions)
            words = [
                word if word.endswith('_') else word.replace('_', '')
                for word, pos in merged_words
                if word.replace('_', '') not in custom_stopwords
            ]
            tokens.extend(words)

        total_docs.append(tokens)  # í•˜ë‚˜ì˜ ë¬¸ì„œ ì™„ì„±!


# ë¶„ì„
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import matplotlib.pyplot as plt

# TF-IDF ì…ë ¥ì„ ìœ„í•´ "ë¬¸ì¥"ìœ¼ë¡œ ë³€í™˜
docs_as_str = [" ".join(doc) for doc in total_docs]

# ë²¡í„°í™”
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(docs_as_str)
feature_names = vectorizer.get_feature_names_out()

# ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
tfidf_scores = tfidf_matrix.sum(axis=0).A1
sorted_indices = np.argsort(tfidf_scores)[::-1]
top_n = 20
top_keywords = [(feature_names[i], tfidf_scores[i]) for i in sorted_indices[:top_n]]

# ê²°ê³¼ ì¶œë ¥
print("\nğŸ”‘ TF-IDF ìƒìœ„ í‚¤ì›Œë“œ Top 20:")
for word, score in top_keywords:
    print(f"{word}: {score:.4f}")

# ì‹œê°í™”
# keywords, scores = zip(*top_keywords)
# plt.figure(figsize=(10, 6))
# plt.barh(keywords[::-1], scores[::-1], color='coral')
# plt.title("Top 20 Keywords by TF-IDF Score")
# plt.xlabel("TF-IDF Score")
# plt.tight_layout()
# plt.show()