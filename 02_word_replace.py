from kiwipiepy import Kiwi
from kiwipiepy.utils import Stopwords
from konlpy.tag import Okt
from pprint import pprint
import pandas as pd
import re
import csv

# 1. ì œëª©ì˜ ì‰¼í‘œ, ì´ëª¨í‹°ì½˜, íŠ¹ìˆ˜ë¬¸ì ë“± ì‚­ì œ
def clean_title(text):
    text = str(text)
    text = re.sub(r'[\U00010000-\U0010ffff]', '', text)  # ì´ëª¨ì§€ ì œê±°
    text = text.replace(',', '')                         # ì‰¼í‘œ ì œê±°
    text = re.sub(r'[^\w\s]', '', text)                  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    text = text.strip()                                  # ì•ë’¤ ê³µë°± ì œê±°
    return text


# ê³ ìœ ì–´ ì‚¬ì „
custom_exceptions = [
    'ê°•ì°¬ì„', 'ì§€ë°•ë ¹', 'ì‚¼ì„±ì „ì', 'ì„œìš¸íŠ¹ë³„ì‹œ', 'ì¹´ì¹´ì˜¤ì—”í„°í”„ë¼ì´ì¦ˆ', 'TVì¡°ì„ ', 'ì±„ë„A'
]
def preserve_phrases(text, phrase_list):
    for phrase in phrase_list:
        if phrase in text:
            text = text.replace(phrase, phrase.replace(' ', '_'))  # ë„ì–´ì“°ê¸° ìˆëŠ” ê²½ìš°ë„ ëŒ€ë¹„
    return text

# í˜•íƒœì†Œ ë¶„ì„ + ë¶ˆìš©ì–´ ì œê±° í•¨ìˆ˜ (konlpy ì‚¬ìš©)
def tokenize_and_remove_stopwords(text):
    okt = Okt()
    stopwords = set([
        'ì˜', 'ê°€', 'ì´', 'ì€', 'ë“¤', 'ëŠ”', 'ì¢€', 'ì˜', 'ê±', 'ê³¼', 'ë„', 'ë¥¼',
        'ìœ¼ë¡œ', 'ì', 'ì—', 'ì™€', 'í•œ', 'í•˜ë‹¤', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ê³ ', 'ë„',
        'ì—ê²Œ', 'ê·¸ë¦¬ê³ ', 'ê·¸', 'ê²ƒ', 'ë¡œ', 'ã…‹ã…‹', 'ã… ã… ', 'ê·¸ë˜ë„', 'ë„ˆë¬´', 'ì–´ëŠ',
        'ì œ', 'ì£ ', 'ì„'
    ])
    text = preserve_phrases(text, custom_exceptions)
    tokens = okt.morphs(text, stem=True)
    tokens = [token.replace('_', '') for token in tokens]
    filtered_tokens = [token for token in tokens if token not in stopwords]
    return filtered_tokens

# ì½ê¸° ë° ì²˜ë¦¬
with open('/home/ubuntu/damf2/yootube/data/2025-04-24.csv', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)

    for i, row in enumerate(reader):
        raw_title = row['title']
        cleaned_title = clean_title(raw_title)
        tokens_title = tokenize_and_remove_stopwords(cleaned_title)
    
        raw_description = row['description']
        cleaned_description = clean_title(raw_description)
        tokens_description = tokenize_and_remove_stopwords(cleaned_description)

        print(f"ğŸ“Œ\n[{i+1}ë²ˆ ì œëª©]")
        print(f"ì›ë¬¸: {raw_title}")
        print(f"ì •ì œ: {cleaned_title}")
        print(f"í† í°: {tokens_title}")
        print(f"ğŸ“Œ\n[{i+1}ë²ˆ ì„¤ëª…]")
        print(f"ì›ë¬¸: {raw_description}")
        print(f"ì •ì œ: {cleaned_description}")
        print(f"í† í°: {tokens_description}")
        print('----------------')

# 3. CSV íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬
# df = pd.read_csv('/home/ubuntu/damf2/yootube/data/2025-04-24.csv', encoding='utf-8')
# # print(df.columns)

# # 4. ì œëª© ì •ì œ ë° í† í°í™” ì ìš©
# df['title'] = df['title'].apply(clean_title)
# df['tokens'] = df['title'].apply(tokenize_and_remove_stopwords)

# print(df['tokens'].head(2))

# # 5. csv íŒŒì¼ ì €ì¥ í•¨ìˆ˜
# local_dir = '/home/ubuntu/damf2/yootube/data'
# today = datetime.today().strftime('%Y-%m-%d')
# file_path = os.path.join(local_dir, f'{today}.csv')

# # ì•„ì§ í•˜ëŠ” ì¤‘
# def save_to_csv(data):
#     with open(file_path, 'w', encoding='utf-8-sig', newline='') as file:
#         writer = csv.DictWriter(file, fieldnames=['channel', 'view_count', 'title', 'description'])
#         writer.writeheader()        # í—¤ë” ì¶”ê°€
#         writer.writerows(data)     # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ì €ì¥
#     print(f'âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}')

# if __name__ == '__main__':
#     video_data = get_trending_videos()
#     save_to_csv(video_data)

# 5. í™•ì¸
# for i in range(5):
#     print(f"{i}ë²ˆ:\nì›ë¬¸: {df['title'][i]}\ní† í°: {df['tokens'][i]}\n")
#     print(f"{i}ë²ˆ:\nì›ë¬¸: {df['description'][i]}\ní† í°: {df['tokens'][i]}\n")